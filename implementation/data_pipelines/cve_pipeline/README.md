# ğŸ›¡ CVE Pipeline Missing lots of modification for this pipeline

## ğŸ“Œ Table of Contents
- [ğŸ” Project Overview](#-project-overview)
- [ğŸš§ Current Technical & Budget Constraints](#-current-technical--budget-constraints)
- [ğŸš€ Final Goals](#-final-goals)
- [ğŸ Competitors](#-competitors)
- [â—Key Technical Challenges & Roadblocks](#ï¸-key-technical-challenges--roadblocks)
- [ğŸ’¡ Proposed Solutions](#-proposed-solutions)
- [ğŸ“ˆ System Architecture](#-system-architecture)
- [ğŸ”§ Features](#-features)
- [ğŸ§ª Pipeline Phases](#-pipeline-phases)
- [ğŸ§¬ Data Flow Diagram](#-data-flow-diagram)
- [ğŸ—‚ Directory Structure](#-directory-structure)
- [ğŸ“¦ Tech Stack](#-tech-stack)

---

## ğŸ” Project Overview
The **CVE Pipeline** is a threat intelligence ingestion system that automatically collects and processes daily vulnerability data from the NVD JSON Feeds, merging recent vulnerabilities and modifications into both operational and historical datasets.  
This system supports **real-time monitoring**, **automated data cleaning**, and **integration into TIP (Threat Intelligence Platform)** for **NLP-based vulnerability analysis**.

---

## ğŸš§ Current Technical & Budget Constraints
- **Orchestration**: Using **Prefect** for workflow orchestration.  
- **CI/CD**: **GitHub Actions** for automated runs.  
- **Budget**: Zero-cost infrastructure preferred â€” leveraging GitHub storage and free-tier cloud/big data solutions.  
- **Limitations**: No large paid cloud database; restricted to free-tier / self-hosted / open-source tools.

---

## ğŸš€ Final Goals
- Real-time ingestion of **NVD CVE feeds** (recent + modified).
- Maintain **cumulative historical CVE database**.
- Auto-tagging vulnerabilities with **CWE**, **CVSS scores**, **MITRE ATT&CK mapping**, and **Exploit data**.
- Serve **processed datasets** for integration with TIP and NLP assistants.
- Enable **API access** for other security tools.

---

## ğŸ Competitors
- **Vulners API**
- **CIRCL CVE Search**
- **Rapid7 Vulnerability DB**
- **CVE Details**
- **MITREâ€™s CVE API**

---

## â—Key Technical Challenges & Roadblocks
1. **Historical + real-time sync** without duplications.
2. Handling **updated CVEs for older vulnerabilities**.
3. Large-scale data storage in a **budget-friendly way**.
4. **Free-tier hosting limitations** for big data.
5. Maintaining **data schema consistency**.

---

## ğŸ’¡ Proposed Solutions
- Use **cumulative + operational table design** to merge historical and daily feeds.
- **Deduplication logic** in ETL layer.
- Store structured data in **DuckDB** or **SQLite** for local and GitHub-based access.
- Use **Parquet** format for efficient GitHub storage and retrieval.
- Use **Prefect** to orchestrate daily ingestion + merging.
- Integrate **GitHub Actions** for scheduled automation.
---

## ğŸ“ˆ System Architecture
![System Architecture Diagram](docs/architecture_diagram.png)

---

## ğŸ”§ Features

- Automated ingestion of **daily CVE data**.
- Handles both **new** and **modified** vulnerabilities.
- **Schema mapping** to CWE, CVSS, ATT&CK, ExploitDB.
- Efficient **storage format** for free-tier limits.
- **Orchestrated** with Prefect.
- **Versioned** via GitHub.

---

## ğŸ§ª Pipeline Phases

<details>
<summary>âœ… Phase 1: Reddit Data Extraction (Scraping)</summary>

- ğŸ” **Inputs:**
  - List of subreddits
  - Reddit API credentials (via `praw`)
  - Configs (e.g., number of posts, filters)

- âš™ï¸ **Inside:**
  - Fetch top daily/weekly posts with comments
  - Remove posts with no comments
  - Filter spam/bot content
  - Save results in `/data/raw/` as JSON or CSV

- ğŸ¯ **Purpose:**  
  Collect relevant raw text data (real-world issues and discussions) for downstream LLM processing.

- ğŸ” **Used again in:**  
  Phase 2 (Cleaning), Phase 10 (Re-training or evaluation for LLMs)

- ğŸ“¤ **Outputs:**  
  `/data/raw/reddit_posts_with_comments.json`

</details>

<details>
<summary>âœ… Phase 2: Reddit Data Cleaning (LLM-Based)</summary>

- ğŸ” **Inputs:**  
  - Raw Reddit posts + top comments  
  - LLM model (offline or Ollama)  
  - Prompt template

- âš™ï¸ **Inside:**  
  - Preprocessing: Remove bots, normalize text  
  - LLM Inference: Extract (problem â†’ solution) pairs using prompts  
  - Postprocessing: JSON formatting, hallucination checks, null handling

- ğŸ¯ **Purpose:**  
  Converts noisy internet content into clean problemâ€“solution pairs for chatbot and tagging.

- ğŸ” **Used again in:**  
  Phase 3 (Tag Generation), Phase 9 (LLM chatbot fine-tuning)

- ğŸ“¤ **Outputs:**  
  `/data/cleaned/cleaned_problems_solutions.json`

</details>

<details>
<summary>ğŸ¦‘ Phase 3: Data Augmentation & Translation</summary>

- ğŸ” **Inputs:**  
  - Cleaned problemâ€“solution pairs  
  - NLPAug/TextAttack or offline LLMs for paraphrasing  
  - Optional translation APIs or offline models  
  - Noise injection rules (typos, slang)

- âš™ï¸ **Inside:**  
  - Paraphrasing: Generate 1â€“3 semantically similar versions  
  - Translation: Translate â†’ Back-translate (e.g., EN â†’ AR â†’ EN)  
  - Noise Injection: Add typos, abbreviations  
  - Flow management: `augmenter/flow.py`, `translator/flow.py`

- ğŸ¯ **Purpose:**  
  Increase data diversity and robustness to phrasing variability and multilingual input.

- ğŸ” **Used again in:**  
  Phase 5 (Embedding generation), Phase 10 (Chatbot understanding)

- ğŸ“¤ **Outputs:**  
  `/data/augmented/augmented_problems_solutions.json`

</details>

<details>
<summary>ğŸŒ¿ Phase 4: Tag Generator (Problem + Solution Tags)</summary>

- ğŸ” **Inputs:**  
  - Cleaned or augmented problemâ€“solution pairs  
  - Tagging rules or LLM model  
  - Optional keyword dictionaries or tag schemas

- âš™ï¸ **Inside:**  
  - Extract semantic tags from problems and solutions  
  - Track source (rule-based, LLM, or hybrid)  
  - Store metadata like confidence, LLM version  
  - Orchestrated via `tag_generator/flow.py`

- ğŸ¯ **Purpose:**  
  Enables structured understanding for tag-based filtering and scoring in recommendations.

- ğŸ” **Used again in:**  
  Phase 6 (Tag-based matching), Phase 10 (Chatbot explanations)

- ğŸ“¤ **Outputs:**  
  `/data/tagged/tagged_problems_solutions.json`

</details>

<details>
<summary>ğŸ”¢ Phase 5: Embedding Generation (Problems + Branches)</summary>

- ğŸ” **Inputs:**  
  - Cleaned/tagged problemâ€“solution pairs  
  - Branch expertise descriptions  
  - Pretrained embedding model (e.g., Sentence-BERT, Instructor-XL)

- âš™ï¸ **Inside:**  
  - Vectorize problemâ€“solution pairs  
  - Vectorize branch expertise profiles  
  - Store embeddings separately (`/data/embeddings/problems/`, `/data/embeddings/branches/`)  
  - Auto-skip already embedded entries  
  - Freeze model versions & store hashes  
  - Flow handled by `embedding_generator/flow.py`

- ğŸ¯ **Purpose:**  
  Enables similarity-based retrieval and matching for hybrid recommendations.

- ğŸ” **Used again in:**  
  Phase 6 (Similarity scoring), Phase 10 (Chatbot reasoning)

- ğŸ“¤ **Outputs:**  
  `/data/embeddings/problems/*.npy`, `/data/embeddings/branches/*.npy`

</details>

<details>
<summary>ğŸ—ºï¸ Phase 6: Branch Recommender System</summary>

- ğŸ” **Inputs:**  
  - Tagged problems  
  - Problem embeddings  
  - Branch embeddings + tag profiles  
  - Branch availability + location (optional)

- âš™ï¸ **Inside:**  
  - Match tags (e.g., Jaccard Index)  
  - Match vectors (cosine similarity)  
  - Apply location filter if coordinates provided  
  - Composite scoring: weighted formula of tags, embeddings, location  
  - Return top-N recommendations with explainability logs  
  - Flow: `branch_recommender/flow.py`

- ğŸ¯ **Purpose:**  
  Core logic to choose the best-fit repair branch per user query.

- ğŸ” **Used again in:**  
  Phase 10 (Chatbot resolution), Phase 11 (Backend endpoint)

- ğŸ“¤ **Outputs:**  
  `/data/recommendations/top_branches_for_postid.json`

</details>

<details>
<summary>ğŸ§ª Phase 7: Local & Integrated Testing</summary>

- ğŸ” **Inputs:**  
  - Outputs from previous phases  
  - Small manually crafted test batch  
  - Expected results/ground truth (if available)

- âš™ï¸ **Inside:**  
  - Run unit tests per script  
  - Run integration tests on test batch  
  - Visualize embeddings, matches, tags  
  - Store snapshots in `/docs/test_cases/`

- ğŸ¯ **Purpose:**  
  Verify correctness and integration before scaling.

- ğŸ” **Used again in:**  
  Phase 12 (Documentation), CI/CD (Phase 9)

- ğŸ“¤ **Outputs:**  
  `/docs/test_cases/*.json`, `/docs/test_results/`, visuals

</details>

<details>
<summary>ğŸŒ€ Phase 8: Prefect Orchestration</summary>

- ğŸ” **Inputs:**  
  - All flow.py scripts (Phases 1â€“6)  
  - Prefect config (retry, logging)  
  - Optional Prefect Cloud credentials

- âš™ï¸ **Inside:**  
  - Convert scripts to Prefect tasks  
  - Chain tasks in logical order  
  - Add retries, error handlers, logging  
  - Trigger from CLI or schedule

- ğŸ¯ **Purpose:**  
  Automate and connect pipeline parts in modular robust system.

- ğŸ” **Used again in:**  
  Phase 9 (CI/CD), Phase 11 (Runtime scheduling)

- ğŸ“¤ **Outputs:**  
  Prefect DAG, CLI runnable flows, logs

</details>

<details>
<summary>â˜ï¸ Phase 9: GitHub Actions & Deployment</summary>

- ğŸ” **Inputs:**  
  - GitHub repo + workflows  
  - Prefect-compatible flows  
  - Secrets (.env or GitHub Secrets)

- âš™ï¸ **Inside:**  
  - Run flows (scraping, cleaning, tagging, embedding, matching)  
  - Scheduled daily (e.g., 12:15 PM Egypt time)  
  - Support matrix builds and parallelization  
  - Optional Docker container builds

- ğŸ¯ **Purpose:**  
  Fully automated data ingestion & processing pipeline on GitHub infrastructure.

- ğŸ” **Used again in:**  
  All phases (1â€“6), redeployment on code updates

- ğŸ“¤ **Outputs:**  
  Daily updated `/data/`, GitHub CI logs, optional Docker images

</details>

<details>
<summary>ğŸ“˜ Phase 10: LLM Chatbot Engine</summary>

- ğŸ” **Inputs:**  
  - User query (via REST API)  
  - Cleaned + tagged Reddit problems  
  - Embeddings (problems & branches)  
  - Branch metadata (tags, location)

- âš™ï¸ **Inside:**  
  - Classify query intent  
  - Retrieve similar Reddit cases  
  - LLM generates structured response  
  - Match to branch via Phase 6 logic  
  - Format JSON response for chatbot

- ğŸ¯ **Purpose:**  
  Frontline AI interaction interface.

- ğŸ” **Used again in:**  
  Phase 11 (API routes), Phase 12 (docs)

- ğŸ“¤ **Outputs:**  
  Structured JSON `{ "solution": ..., "branch": ..., "confidence": ... }`

</details>

<details>
<summary>ğŸšª Phase 11: Backend Integration (FastAPI)</summary>

- ğŸ” **Inputs:**  
  - Chatbot logic (Phase 10)  
  - Recommender logic (Phase 6)  
  - Processed data (embeddings, tags)  
  - API config & schema

- âš™ï¸ **Inside:**  
  - REST endpoints: `/chat/solve`, `/recommend/branch`  
  - Parse inputs, run logic, return JSON  
  - Dockerized for modular deployment  
  - Optional Redis caching

- ğŸ¯ **Purpose:**  
  Expose system via API for production apps.

- ğŸ” **Used again in:**  
  Real-time deployment, frontend integration

- ğŸ“¤ **Outputs:**  
  `main.py` FastAPI server, OpenAPI docs

</details>

<details>
<summary>ğŸ“˜ Phase 12: Documentation & Finalization</summary>

- ğŸ” **Inputs:**  
  - All code, flows, configs, data samples  
  - Testing results (Phase 7)  
  - Model & LLM choices

- âš™ï¸ **Inside:**  
  - Create README, architecture diagrams  
  - Document phases and modules  
  - Glossary, schema definitions  
  - Data samples & test outputs

- ğŸ¯ **Purpose:**  
  Make pipeline shareable, reproducible, production-ready.

- ğŸ” **Used again in:**  
  Onboarding, public release, presentations

- ğŸ“¤ **Outputs:**  
  `/docs/`, `README.md`, diagrams, prompt designs, schema

</details>

---

## ğŸ§¬ Data Flow Diagram

<p align="center">
  <img src="docs/data_flow_diagram.png" alt="Data Flow Diagram" width="30%" />
</p>

---

## ğŸ—‚ Directory Structure
- cve_pipeline/
  - flows/
    - __init__.py
    - run_cve_pipeline.py       # Main Prefect flow entrypoint
    - cve_pipeline_flow.py      # Alternative flow definition / modularized steps
  - ingestion/
    - __init__.py
    - cve_ingestor.py           # Pulls CVE JSON data from NVD API
    - cve_yearly_checker.py     # Checks for missing yearly files
    - cve_new_checker.py        # Checks for new CVEs since last run
    - cve_modified_checker.py   # Checks for modified CVEs
    - fetch_recent.py           # Fetches most recent CVE records
    - fetch_modified.py         # Fetches recently modified CVEs
  - processing/
    - __init__.py
    - normalize_cve.py          # Normalizes CVE JSON into a tabular format
    - update_cumulative.py      # Updates historical cumulative DB
    - parse_cve.py              # Parses CVE JSON into structured fields
    - merge_datasets.py         # Merges multiple CVE datasets
  - storage/
    - __init__.py
    - file_manager.py           # Saves to local CSV/Parquet
    - db_manager.py             # Handles cumulative DB logic
    - save_to_parquet.py        # Saves processed data to Parquet format
    - save_to_sqlite.py         # Saves processed data to SQLite database
  - utils/
    - __init__.py
    - api_client.py             # NVD API calls & retries
    - logger.py                 # Central logging config
    - config.py                 # Settings (env variables)
  - tests/
    - __init__.py
    - test_ingestion.py
    - test_processing.py
  - data/
    - raw/                      # Raw JSON downloads (by year)
    - processed/                # Normalized CSV/Parquet files
    - cumulative/               # Historical full dataset
  - requirements.txt
  - README.md
  - .github/
    - workflows/
      - cve_pipeline.yml        # GitHub Actions workflow for automation


---

## ğŸ“¦ Tech Stack

| Category        | Tool / Library                |
|-----------------|------------------------------|
| Data Source     | Reddit (PRAW, HTTPX)         |
| LLM             | Ollama hosted on GitHub      |
| Embedding Models| SBERT, Instructor-XL         |
| Tagging         | LLM-driven & rule-based      |
| Workflow        | Prefect + GitHub Actions     |
| API Backend     | FastAPI, Pydantic            |
| Scraping        | PRAW, HTTPX                  |
| Containerization| Docker, Docker Compose       |
| Language        | Python 3.10+                 |
| Storage         | GitHub, PostgreSQL(Future)   |

---

## ğŸ§  Prompt Engineering Principles

- Clear role and intent definitions  
- JSON-structured outputs for consistent parsing  
- Resistant to hallucinations with carefully designed examples  
- Few-shot learning and instruction-based prompts  
- Flexible for multi-language support and augmentation

See [docs/prompt_templates.pdf](/docs/prompt_templates.pdf) for detailed templates.

---

## ğŸ—“ Roadmap
## ğŸ—“ Roadmap  
*(TBD = To Be Determined)*

| Phase | Description | Start Date | End Date | Status |
|-------|-------------|------------|----------|--------|
| âœ… 1  | Reddit Data Extraction (Scraping) | 2025-07-15 | 2025-07-30 | âœ… Done |
| âœ… 2  | Reddit Data Cleaning (LLM-Based) | 2025-08-01 | 2025-08-10 | âœ… Done |
| ğŸ¦‘ 3  | Data Augmentation & Translation | TBD | TBD | ğŸ”„ Planned |
| ğŸŒ¿ 4  | Tag Generator (Problem + Solution Tags) | TBD | TBD | ğŸ”„ Planned |
| ğŸ”¢ 5  | Embedding Generation (Problems + Branches) | TBD | TBD | ğŸ”„ Planned |
| ğŸ—ºï¸ 6  | Branch Recommender System | TBD | TBD | ğŸ”„ Planned |
| ğŸ§ª 7  | Local & Integrated Testing | TBD | TBD | ğŸ”„ Planned |
| ğŸŒ€ 8  | Prefect Orchestration | 2025-07-15 | Ongoing | ğŸ”„ In Progress |
| â˜ï¸ 9  | GitHub Actions & Deployment | TBD | TBD | ğŸ”„ Planned |
| ğŸ“˜ 10 | LLM Chatbot Engine | TBD | TBD | ğŸ”„ Planned |
| ğŸšª 11 | Backend Integration (FastAPI) | TBD | TBD | ğŸ”„ Planned |
| ğŸ“˜ 12 | Documentation & Finalization | 2025-07-15 | Ongoing | ğŸ”„ In Progress |

---

## ğŸ§¾ License
No license has been selected for this project yet.  
All rights reserved â€” you may not use, copy, modify, or distribute this code without explicit permission from the author.

---

## ğŸ‘¨â€ğŸ’» Author
**Ibrahim Hegazi**  
_Data Engineer â€¢ AI Engineer â€¢ NLP & ML Enthusiast â€¢ System Designer_  

ğŸŒ [GitHub](https://github.com/Ibrahim-Hegazi) | [LinkedIn](https://www.linkedin.com/in/ibrahim-hegazi/)

---

## ğŸ“¬ Future Improvements

- **Cloud Deployment & Scalability**
  - Deploy pipeline and API services to **GCP**, **Azure**, or **AWS**
  - Use serverless functions for on-demand processing
  - Enable auto-scaling for high-traffic periods

- **Integration with Car Clinicâ€™s Internal Systems**
  - Connect to **CRM** for seamless ticket creation and tracking
  - Sync with internal repair history databases
  - Enable push notifications for branch managers

- **Model Enhancements**
  - Implement continuous **feedback loops** for retraining
  - Expand **multilingual support** beyond Arabic & Spanish
  - Integrate domain-specific fine-tuning for higher tagging accuracy

- **Observability & Monitoring**
  - Real-time metrics dashboard with **Grafana**
  - Alerting system via **Prometheus** and custom webhooks
  - Automated anomaly detection for unusual patterns in requests

- **User Experience Improvements**
  - Build **interactive frontend UI**
  - Mobile-first responsive design
  - Chatbot enhancements for context memory and proactive suggestions

- **Security & Compliance**
  - Implement **API rate limiting** and authentication
  - Ensure **GDPR-compliant** data handling
  - Add role-based access controls for internal users

---

## ğŸ™‹â€â™‚ï¸ Contributing
Contributions are welcome! Please open an issue first to discuss any proposed changes.

---

## ğŸ“ Contact

For questions or support, please reach out to:  

**Ibrahim Hegazi**  
ğŸ“§ Email: [ihegaziwork@gmail.com](mailto:ihegaziwork@gmail.com)  
ğŸ™ GitHub: [Ibrahim-Hegazi](https://github.com/Ibrahim-Hegazi)  
ğŸ’¼ LinkedIn: [Ibrahim Hegazi](https://www.linkedin.com/in/ibrahim-hegazi/)
